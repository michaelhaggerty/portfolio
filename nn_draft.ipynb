{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the RGB values for red, yellow, and blue\n",
    "data = [\n",
    "    ([255, 0, 0], [1, 0, 0]),    # Red\n",
    "    ([254, 0, 0], [1, 0, 0]),    # Red\n",
    "    ([255, 255, 0], [0, 1, 0]),  # Yellow\n",
    "    ([255, 254, 0], [0, 1, 0]),  # Yellow\n",
    "    ([0, 0, 255], [0, 0, 1]),    # Blue\n",
    "    ([0, 0, 254], [0, 0, 1]),    # Blue\n",
    "]\n",
    "\n",
    "# Normalize RGB values (0-1 range)\n",
    "data = [([r/255, g/255, b/255], label) for (r, g, b), label in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Test sigmoid and sigmoid_derivative\n",
    "print(sigmoid(0))  # Should print 0.5\n",
    "print(sigmoid_derivative(0.5))  # Should print 0.25 (since sigmoid(0) = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "def dot_product(a, b):\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "# Test dot_product\n",
    "print(dot_product([1, 2, 3], [4, 5, 6]))  # Should print 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 122]\n"
     ]
    }
   ],
   "source": [
    "def matrix_vector_product(matrix, vector):\n",
    "    return [dot_product(row, vector) for row in matrix]\n",
    "\n",
    "# Test matrix_vector_product\n",
    "matrix = [[1, 2, 3], [4, 5, 6]]\n",
    "vector = [7, 8, 9]\n",
    "print(matrix_vector_product(matrix, vector))  # Should print [50, 122]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "def vector_add(a, b):\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "# Test vector_add\n",
    "print(vector_add([1, 2, 3], [4, 5, 6]))  # Should print [5, 7, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights (wh): [[-0.04615633028645072, 0.155833073592462, 0.6917146426846332, 0.9852553442439274, 0.7743329406293673], [-0.061987830407570055, 0.8952514633427648, 0.0839731957063432, 0.9413137087295205, -0.8877563852834292], [0.21054938233171638, -0.9924957167615076, -0.4573148559274707, 0.41003429046669626, -0.3466075803018678]]\n",
      "Initial biases (bh): [-0.16337247950523026, 0.9699008024113875, 0.20920310142668108, 0.29774831701301685, -0.17703942872311784]\n",
      "Initial weights (wout): [[-0.34655252870546516, -0.019731448581209454, -0.9135302374014402], [0.670520491389293, 0.21086996418740545, 0.4732786899981447], [-0.25949104354503727, 0.12092109641407722, 0.06342740157999316], [-0.5726874653001537, 0.8782303012192321, -0.9291009939603219], [-0.9210676961523838, 0.04919300405556415, -0.2972492434207965]]\n",
      "Initial biases (bout): [0.5163852708685734, 0.5502984312327261, 0.1634655578300812]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        self.wh = [[random.uniform(-1, 1) for _ in range(hidden_neurons)] for _ in range(input_neurons)]\n",
    "        self.bh = [random.uniform(-1, 1) for _ in range(hidden_neurons)]\n",
    "        self.wout = [[random.uniform(-1, 1) for _ in range(output_neurons)] for _ in range(hidden_neurons)]\n",
    "        self.bout = [random.uniform(-1, 1) for _ in range(output_neurons)]\n",
    "\n",
    "# Initialize a small neural network and print weights and biases to verify\n",
    "nn = NeuralNetwork(input_neurons=3, hidden_neurons=5, output_neurons=3)\n",
    "print(\"Initial weights (wh):\", nn.wh)\n",
    "print(\"Initial biases (bh):\", nn.bh)\n",
    "print(\"Initial weights (wout):\", nn.wout)\n",
    "print(\"Initial biases (bout):\", nn.bout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden activation: [[0.6253084715187526, 0.26045965544771976, 0.2242521598897799]]\n",
      "Output: [[0.531563903878311, 0.4438752901429513, 0.6039053354209674]]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        self.wh = [[random.uniform(-1, 1) for _ in range(hidden_neurons)] for _ in range(input_neurons)]\n",
    "        self.bh = [random.uniform(-1, 1) for _ in range(hidden_neurons)]\n",
    "        self.wout = [[random.uniform(-1, 1) for _ in range(output_neurons)] for _ in range(hidden_neurons)]\n",
    "        self.bout = [random.uniform(-1, 1) for _ in range(output_neurons)]\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        hidden_input = [vector_add(matrix_vector_product(self.wh, x), self.bh) for x in X]\n",
    "        hidden_activation = [[sigmoid(x) for x in h_inp] for h_inp in hidden_input]\n",
    "        output_input = [vector_add(matrix_vector_product(self.wout, h_act), self.bout) for h_act in hidden_activation]\n",
    "        output = [[sigmoid(x) for x in out_inp] for out_inp in output_input]\n",
    "        return hidden_activation, output\n",
    "\n",
    "# Test forward propagation\n",
    "nn = NeuralNetwork(input_neurons=3, hidden_neurons=5, output_neurons=3)\n",
    "X = [[0.5, 0.5, 0.5]]  # Example input\n",
    "hidden_activation, output = nn.forward_propagation(X)\n",
    "print(\"Hidden activation:\", hidden_activation)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Initialize and train the neural network\u001b[39;00m\n\u001b[1;32m     79\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(input_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Test the neural network\u001b[39;00m\n\u001b[1;32m     83\u001b[0m test_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     84\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],      \u001b[38;5;66;03m# Red\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],      \u001b[38;5;66;03m# Yellow\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]   \u001b[38;5;66;03m# Should be close to Red or Blue\u001b[39;00m\n\u001b[1;32m     89\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     54\u001b[0m     hidden_activation, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_propagation(X)\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_activation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mNeuralNetwork.backpropagation\u001b[0;34m(self, X, y, hidden_activation, output, learning_rate)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwout)):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwout[\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwout[j][k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mhidden_activation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m d_output[k] \u001b[38;5;241m*\u001b[39m learning_rate\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbout)):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbout[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_output[j] \u001b[38;5;241m*\u001b[39m learning_rate\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def dot_product(a, b):\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "def matrix_vector_product(matrix, vector):\n",
    "    return [dot_product(row, vector) for row in matrix]\n",
    "\n",
    "def vector_add(a, b):\n",
    "    return [x + y for x, y in zip(a, b)]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        self.wh = [[random.uniform(-1, 1) for _ in range(hidden_neurons)] for _ in range(input_neurons)]\n",
    "        self.bh = [random.uniform(-1, 1) for _ in range(hidden_neurons)]\n",
    "        self.wout = [[random.uniform(-1, 1) for _ in range(output_neurons)] for _ in range(hidden_neurons)]\n",
    "        self.bout = [random.uniform(-1, 1) for _ in range(output_neurons)]\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        hidden_input = [vector_add(matrix_vector_product(self.wh, x), self.bh) for x in X]\n",
    "        hidden_activation = [[sigmoid(x) for x in h_inp] for h_inp in hidden_input]\n",
    "        output_input = [vector_add(matrix_vector_product(self.wout, h_act), self.bout) for h_act in hidden_activation]\n",
    "        output = [[sigmoid(x) for x in out_inp] for out_inp in output_input]\n",
    "        return hidden_activation, output\n",
    "\n",
    "    def backpropagation(self, X, y, hidden_activation, output, learning_rate=0.1):\n",
    "        for i in range(len(X)):\n",
    "            error = [y[i][j] - output[i][j] for j in range(len(y[i]))]\n",
    "            d_output = [error[j] * sigmoid_derivative(output[i][j]) for j in range(len(error))]\n",
    "\n",
    "            hidden_error = [dot_product(d_output, [self.wout[k][j] for k in range(len(self.wout))]) for j in range(len(self.wout[0]))]\n",
    "            d_hidden = [hidden_error[j] * sigmoid_derivative(hidden_activation[i][j]) for j in range(len(hidden_error))]\n",
    "\n",
    "            for j in range(len(self.wout)):\n",
    "                for k in range(len(self.wout[0])):\n",
    "                    self.wout[j][k] += hidden_activation[i][j] * d_output[k] * learning_rate\n",
    "            for j in range(len(self.bout)):\n",
    "                self.bout[j] += d_output[j] * learning_rate\n",
    "            for j in range(len(self.wh)):\n",
    "                for k in range(len(self.wh[0])):\n",
    "                    self.wh[j][k] += X[i][j] * d_hidden[k] * learning_rate\n",
    "            for j in range(len(self.bh)):\n",
    "                self.bh[j] += d_hidden[j] * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs=10000, learning_rate=0.1):\n",
    "        for _ in range(epochs):\n",
    "            hidden_activation, output = self.forward_propagation(X)\n",
    "            self.backpropagation(X, y, hidden_activation, output, learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, output = self.forward_propagation(X)\n",
    "        return output\n",
    "\n",
    "# Prepare the dataset\n",
    "data = [\n",
    "    ([255, 0, 0], [1, 0, 0]),    # Red\n",
    "    ([254, 0, 0], [1, 0, 0]),    # Red\n",
    "    ([255, 255, 0], [0, 1, 0]),  # Yellow\n",
    "    ([255, 254, 0], [0, 1, 0]),  # Yellow\n",
    "    ([0, 0, 255], [0, 0, 1]),    # Blue\n",
    "    ([0, 0, 254], [0, 0, 1]),    # Blue\n",
    "]\n",
    "\n",
    "# Normalize RGB values (0-1 range)\n",
    "data = [([r/255, g/255, b/255], label) for (r, g, b), label in data]\n",
    "\n",
    "# Separate inputs and outputs\n",
    "X = [d[0] for d in data]\n",
    "y = [d[1] for d in data]\n",
    "\n",
    "# Initialize and train the neural network\n",
    "nn = NeuralNetwork(input_neurons=3, hidden_neurons=5, output_neurons=3)\n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test the neural network\n",
    "test_data = [\n",
    "    [1, 0, 0],      # Red\n",
    "    [1, 1, 0],      # Yellow\n",
    "    [0, 0, 1],      # Blue\n",
    "    [0.5, 0.5, 0],  # Should be close to Yellow\n",
    "    [0.5, 0, 0.5]   # Should be close to Red or Blue\n",
    "]\n",
    "\n",
    "# Normalize test data\n",
    "test_data_normalized = [[r, g, b] for [r, g, b] in test_data]\n",
    "predictions = nn.predict(test_data_normalized)\n",
    "\n",
    "# Print the results\n",
    "for i, color in enumerate(test_data):\n",
    "    predicted_label = predictions[i]\n",
    "    label_name = [\"Red\", \"Yellow\", \"Blue\"]\n",
    "    predicted_color = label_name[predicted_label.index(max(predicted_label))]\n",
    "    print(f\"Input RGB: {color} -> Predicted Output: {predicted_color}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "New situation: input data =  1 1 1\n",
      "Output data: \n",
      "[0.99211997]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # Set synaptic weights to a 3x1 matrix,\n",
    "        # with values from -1 to 1 and mean 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        for iteration in range(training_iterations):\n",
    "            # Pass training set through the neural network\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            # Calculate the error rate\n",
    "            error = training_outputs - output\n",
    "\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights += adjustments\n",
    "\n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize the single neuron neural network\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set, with 4 examples consisting of 3\n",
    "    # input values and 1 output value\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    # Train the neural network\n",
    "    neural_network.train(training_inputs, training_outputs, 10000)\n",
    "\n",
    "    print(\"Synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    A = str(input(\"Input 1: \"))\n",
    "    B = str(input(\"Input 2: \"))\n",
    "    C = str(input(\"Input 3: \"))\n",
    "    \n",
    "    print(\"New situation: input data = \", A, B, C)\n",
    "    print(\"Output data: \")\n",
    "    print(neural_network.think(np.array([A, B, C])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
